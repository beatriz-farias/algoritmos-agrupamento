# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O6JUwMkOsOtbt4BcHkKJf_lB4UBanKQY

# Comparação de Algoritmos de Agrupamento

Implementação e comparação de três algoritmos de agrupamento não supervisionado: Kohonen (SOM), K-means e DBSCAN, utilizando o conjunto de dados (Iris Dataset).

## Integrantes do grupo

- Beatriz Farias do Nascimento - 122053127
- Natan Azevedo Gomes - 122047435
- Thomas Cardoso de Miranda - 122050797

## Bibliotecas
"""

from sklearn.datasets import load_iris
import pandas as pd
import numpy as np
import math
import random
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.lines import Line2D
import matplotlib.patches as mpatches
import seaborn as sns
from collections import deque
from sklearn.preprocessing import StandardScaler
import matplotlib.ticker as ticker

"""## Carregamento do dataset"""

# Carregar o dataset Iris e pré-processar
iris = load_iris()
X = iris.data
y = iris.target
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X) # Usar X_scaled para os algoritmos

feature_names = iris.feature_names
target_names = iris.target_names

# Criar um DataFrame base com os dados escalados
df_base = pd.DataFrame(X_scaled, columns=feature_names)
df_base['true_species'] = [target_names[i] for i in y] # Para comparação com o ground truth

# --- Determinar limites globais para os eixos ---
# Encontra o min e max de cada feature em todo o dataset escalado
min_vals = X_scaled.min(axis=0)
max_vals = X_scaled.max(axis=0)

# Adiciona um pequeno padding para que os pontos não fiquem na borda
padding_factor = 0.1 # Ajuste conforme necessário
x_limits = [(min_vals[i] - padding_factor * (max_vals[i] - min_vals[i]),
             max_vals[i] + padding_factor * (max_vals[i] - min_vals[i]))
            for i in range(len(feature_names))]

# Função auxiliar para aplicar os limites e ticks consistentes aos eixos
def set_consistent_axis_limits_and_ticks(g_plot, feature_names, x_limits, major_tick_interval=1.5):
    num_features = len(feature_names)
    for i in range(num_features): # Linhas
        for j in range(num_features): # Colunas
            ax = g_plot.axes[i, j]
            if ax is not None: # VERIFICAÇÃO ADICIONADA: Só procede se o subplot existe
                if i != j: # Apenas para os gráficos de dispersão (fora da diagonal)
                    ax.set_xlim(x_limits[j])
                    ax.set_ylim(x_limits[i])
                    ax.xaxis.set_major_locator(ticker.MultipleLocator(major_tick_interval))
                    ax.yaxis.set_major_locator(ticker.MultipleLocator(major_tick_interval))
                else: # Para os gráficos na diagonal (kde/histograms)
                    ax.set_xlim(x_limits[i])
                    ax.xaxis.set_major_locator(ticker.MultipleLocator(major_tick_interval))


# --- 1. Visualização do Ground Truth (já fornecido) ---
g_gt = sns.pairplot(df_base, hue='true_species', corner=True)
plt.suptitle("Ground Truth - Iris Dataset", y=1.02)
set_consistent_axis_limits_and_ticks(g_gt, feature_names, x_limits)
plt.show()

print("Shape dos dados Iris:", X_scaled.shape)
print("Nome das features:", feature_names)

"""## K-Means

### Lógica do algoritmo

1. Escolha de K valores de forma aleatória como centróides iniciais.
2. Atribuição de cada ponto ao centróide mais próximo (com base na distância euclidiana calculada).
3. Obtenção de um novo centróide pelo cálculo da média dos pontos de cada grupo.
4. Repetir os passos 2 e 3 até a convergência.

### Implementação
"""

def k_means(X_data, k, max_iters=100):
    n_samples = X_data.shape[0]
    centroids = X_data[np.random.choice(n_samples, k, replace=False)]

    for _ in range(max_iters):
        distances = np.linalg.norm(X_data[:, np.newaxis] - centroids, axis=2)
        clusters = np.argmin(distances, axis=1)

        new_centroids = []
        for i in range(k):
            # Lidar com clusters vazios: se um cluster ficar vazio, re-inicialize seu centroide aleatoriamente
            if np.sum(clusters == i) > 0:
                new_centroids.append(X_data[clusters == i].mean(axis=0))
            else:
                new_centroids.append(X_data[np.random.choice(n_samples, 1)].flatten()) # Re-inicializa

        new_centroids = np.array(new_centroids)

        converged = False
        if new_centroids.shape == centroids.shape:
            if np.allclose(centroids, new_centroids):
                converged = True

        centroids = new_centroids
        if converged:
            break

    distances = np.linalg.norm(X_data[:, np.newaxis] - centroids, axis=2)
    final_clusters = np.argmin(distances, axis=1)
    inertia = np.sum((X_data - centroids[final_clusters])**2) # Calcula a inércia com os clusters finais

    return {
        "clusters": final_clusters,
        "metric": inertia,
        "centroids": centroids
    }

"""### Plotagem"""

# --- 2. K-means Visualization with Centroids ---
for k_kmeans in range(2,5):
  kmeans_result = k_means(X_scaled, k=k_kmeans)
  df_kmeans = df_base.copy()
  df_kmeans['kmeans_cluster'] = kmeans_result['clusters']
  centroids_kmeans = kmeans_result['centroids']

  g_kmeans = sns.pairplot(df_kmeans, hue='kmeans_cluster', palette='viridis', corner=True)
  plt.suptitle(f"K-means Clustering (k={k_kmeans}) with Centroids - Iris Dataset", y=1.02)

  # Adicionar os centróides e ajustar limites
  num_features = len(feature_names)
  for i, col_i in enumerate(feature_names):
      for j, col_j in enumerate(feature_names):
          if i > j: # Apenas para os gráficos de dispersão na parte inferior triangular
              ax = g_kmeans.axes[i, j]
              # Verifica se o eixo existe antes de plotar os centróides
              if ax is not None:
                  ax.scatter(centroids_kmeans[:, j], centroids_kmeans[:, i],
                            marker='X', s=50, color='red', edgecolor='black', linewidth=1.5, label='Centroid')
                  for c_idx in range(k_kmeans):
                      ax.text(centroids_kmeans[c_idx, j], centroids_kmeans[c_idx, i], f'C{c_idx+1}',
                              color='black', fontsize=9, ha='center', va='center')

  set_consistent_axis_limits_and_ticks(g_kmeans, feature_names, x_limits)

  handles, labels = g_kmeans.axes[num_features-1, num_features-2].get_legend_handles_labels()

  plt.show()

"""##DBSCAN

###Lógica do Algoritmo

1. Para cada ponto do conjunto de dados, verifica-se se ele já foi visitado.
2. Se ainda não foi visitado, o ponto é marcado como visitado e são identificados seus vizinhos dentro de um raio ε (eps).
3. Se o número de vizinhos for menor que minPts, o ponto é classificado como ruído (noise point).
4. Caso contrário, um novo cluster é iniciado e o ponto atual é marcado como um núcleo (core point).
5. Todos os vizinhos são analisados:
   - Se o vizinho ainda não foi visitado, ele é marcado como visitado e seus próprios vizinhos são verificados.
     Se também for um núcleo, seus vizinhos são adicionados à lista de expansão do cluster.
   - Se o vizinho ainda não pertence a nenhum cluster, ele é adicionado ao cluster atual.
6. Ao final, os pontos que foram adicionados a um cluster, mas não possuem vizinhos suficientes para serem núcleos, são classificados como pontos de borda (border points).
7. O processo é repetido até que todos os pontos tenham sido visitados e classificados.

###Implementação
"""

def region_query(X_data, point_idx, eps):
    neighbors = []
    for j in range(len(X_data)):
        if point_idx != j and np.linalg.norm(X_data[point_idx] - X_data[j]) <= eps:
            neighbors.append(j)
    return neighbors

def expand_cluster(X_data, labels, point_idx, cluster_id, eps, minPts):
    neighbors = region_query(X_data, point_idx, eps)

    # Se o ponto atual não tem vizinhos suficientes (incluindo ele mesmo) para ser um Core Point
    if len(neighbors) + 1 < minPts: # minPts inclui o próprio ponto
        labels[point_idx] = -1  # Marcar como ruído inicial
        return False

    # Se o ponto é um Core Point, atribua-o ao cluster e comece a expandir
    labels[point_idx] = cluster_id
    q = deque(neighbors) # Fila de vizinhos a serem processados

    while q:
        curr_point_idx = q.popleft()

        # Se o ponto foi marcado como ruído anteriormente, reatribuí-lo ao cluster (torna-se um border point)
        if labels[curr_point_idx] == -1:
            labels[curr_point_idx] = cluster_id
        # Se o ponto ainda não foi visitado
        elif labels[curr_point_idx] == 0:
            labels[curr_point_idx] = cluster_id # Atribuí-lo ao cluster atual

            new_neighbors = region_query(X_data, curr_point_idx, eps)

            # Se este novo ponto é também um Core Point, adicione seus vizinhos à fila
            if len(new_neighbors) + 1 >= minPts: # minPts inclui o próprio ponto
                for n_idx in new_neighbors:
                    # Adicione à fila apenas vizinhos que não foram visitados ou foram marcados como ruído
                    if labels[n_idx] == 0 or labels[n_idx] == -1:
                        q.append(n_idx)
            # Se não é um Core Point, mas está no cluster (porque é vizinho de um Core Point), é um Border Point.
            # Seus vizinhos não são processados.

    return True # Cluster expandido com sucesso

def dbscan(X_data, eps=0.5, minPts=5): # minPts padrão ajustado para ser mais comum
    labels = np.zeros(len(X_data), dtype=int)  # 0 = não visitado, -1 = ruído
    cluster_id = 0

    for i in range(len(X_data)):
        if labels[i] != 0: # Se já visitado (clusterizado ou ruído inicial), pule
            continue

        # Tenta expandir um cluster a partir deste ponto
        if expand_cluster(X_data, labels, i, cluster_id + 1, eps, minPts):
            cluster_id += 1 # Novo cluster encontrado

    # Após a fase de expansão, refine a classificação em Core, Border e Noise
    core_points_indices = set()
    border_points_indices = set()
    noise_points_indices = set(np.where(labels == -1)[0]) # Pontos que permaneceram como ruído

    for i in range(len(X_data)):
        if labels[i] > 0: # É parte de um cluster (não é ruído)
            neighbors = region_query(X_data, i, eps)
            # Verifica se é um Core Point com base na definição completa (incluindo ele mesmo)
            if len(neighbors) + 1 >= minPts:
                core_points_indices.add(i)
            else: # Se está em um cluster, mas não é core, é border
                border_points_indices.add(i)

    return {
        "clusters": labels,
        "core_points_indices": list(core_points_indices),
        "border_points_indices": list(border_points_indices),
        "noise_points_indices": list(noise_points_indices),
        "num_clusters": cluster_id
    }

"""### Plotagem"""

# --- 3. DBSCAN Visualization with Core, Border, and Noise ---
eps_dbscan = 0.8
minPts_dbscan = 4

eps_dbscan_list = [0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2]
minPts_dbscan_list = [2,4,6,8,10]
for eps_dbscan in eps_dbscan_list:
  for minPts_dbscan in minPts_dbscan_list:

    dbscan_result = dbscan(X_scaled, eps=eps_dbscan, minPts=minPts_dbscan)
    df_dbscan = df_base.copy()

    dbscan_labeled_clusters = []
    for i, cluster_id in enumerate(dbscan_result['clusters']):
        if cluster_id == -1:
            dbscan_labeled_clusters.append('Noise')
        elif i in dbscan_result['core_points_indices']:
            dbscan_labeled_clusters.append(f'Core_{cluster_id}')
        else:
            dbscan_labeled_clusters.append(f'Border_{cluster_id}')

    df_dbscan['dbscan_cluster_type'] = dbscan_labeled_clusters

    unique_labels_dbscan = sorted(df_dbscan['dbscan_cluster_type'].unique())
    palette_dbscan = {}

    cluster_ids_present = sorted(list(set([int(label.split('_')[1]) for label in unique_labels_dbscan if 'Core' in label or 'Border' in label])))
    color_map_clusters = plt.colormaps.get_cmap('tab10')

    for label_idx, label_name in enumerate(unique_labels_dbscan):
        if label_name == 'Noise':
            palette_dbscan[label_name] = 'gray'
        elif 'Core' in label_name:
            cluster_num = int(label_name.split('_')[1])
            palette_dbscan[label_name] = color_map_clusters(cluster_num)
        elif 'Border' in label_name:
            cluster_num = int(label_name.split('_')[1])
            rgb = color_map_clusters(cluster_num)[:3]
            lighter_rgb = tuple(min(1, x * 1.3) for x in rgb)
            palette_dbscan[label_name] = lighter_rgb

    markers_dbscan = {label: ('o' if 'Core' in label else ('^' if 'Border' in label else 'D'))
                      for label in unique_labels_dbscan}

    default_point_size = 50

    g_dbscan = sns.pairplot(df_dbscan, hue='dbscan_cluster_type', palette=palette_dbscan, corner=True,
                            markers=markers_dbscan,
                            height=1.8, aspect=1.1,
                            plot_kws={'alpha': 0.7, 's': default_point_size})

    plt.suptitle(f"DBSCAN Clustering (eps={eps_dbscan}, minPts={minPts_dbscan}) - Iris Dataset", y=1.02)
    set_consistent_axis_limits_and_ticks(g_dbscan, feature_names, x_limits)
    plt.show()

"""## Kohonen (SOM)

###Lógica do algoritmo
1. Inicialização:
   - A grade de neurônios é criada com dimensões definidas (linhas × colunas).
   - Cada neurônio da grade recebe pesos aleatórios com o mesmo tamanho das entradas dos dados.

2. Para cada época de treinamento:
   - A taxa de aprendizado e o raio de vizinhança (sigma) são decaídos exponencialmente com o tempo.
   
3. Para cada vetor de entrada no conjunto de dados:
   - É identificado o neurônio cuja distância euclidiana entre seus pesos e a entrada é a menor — esse é o chamado BMU (Best Matching Unit).
   
4. Para cada neurônio na grade:
   - Calcula-se a distância entre o neurônio e o BMU na grade (distância de vizinhança).
   - Se essa distância for menor que o raio atual (`sigma`), o neurônio será ajustado.
   - O grau de ajuste é controlado por uma função gaussiana que depende da distância de vizinhança.
   - Os pesos do neurônio são atualizados na direção do vetor de entrada, proporcionalmente à taxa de aprendizado, à influência gaussiana e à diferença entre o vetor de entrada e os pesos do neurônio.

5. O processo se repete por todas as épocas definidas, fazendo com que os neurônios se auto-organizem para refletir a topologia dos dados de entrada.

###Implementação
"""

# ---- Funções auxiliares ---- #

def distancia_euclidiana(v1, v2):
    return np.linalg.norm(np.array(v1) - np.array(v2))

def soma_vetores(v1, v2):
    return [a + b for a, b in zip(v1, v2)]

def subtrai_vetores(v1, v2):
    return [a - b for a, b in zip(v1, v2)]

def multiplica_escalar(escalar, vetor):
    return [escalar * v for v in vetor]

def gaussiana(distancia, sigma):
    return np.exp(- (distancia ** 2) / (2 * sigma ** 2))

# ---- Funções principais ---- #

def inicializar_som(linhas, colunas, tamanho_entrada, taxa_aprendizado=0.1, sigma=1.0, epocas=100):
    np.random.seed(42) # Para reprodutibilidade
    grade = np.random.rand(linhas, colunas, tamanho_entrada) # Pesos entre 0 e 1

    return {
        'linhas': linhas,
        'colunas': colunas,
        'tamanho_entrada': tamanho_entrada,
        'taxa_inicial': taxa_aprendizado,
        'sigma_inicial': sigma,
        'epocas': epocas,
        'grade': grade
    }

def achar_bmu(som, entrada):
    menor_dist = float('inf')
    coordenadas_bmu = (0, 0)
    for i in range(som['linhas']):
        for j in range(som['colunas']):
            pesos = som['grade'][i, j]
            dist = distancia_euclidiana(pesos, entrada)
            if dist < menor_dist:
                menor_dist = dist
                coordenadas_bmu = (i, j)
    return coordenadas_bmu

def distancia_vizinhanca_som(c1, c2):
    return np.sqrt((c1[0] - c2[0])**2 + (c1[1] - c2[1])**2)

def treinar_som(som_model, dados):
    for epoca in range(som_model['epocas']):
        # Declínio linear para taxa de aprendizado e sigma
        taxa = som_model['taxa_inicial'] * (1 - epoca / som_model['epocas'])
        sigma = som_model['sigma_inicial'] * (1 - epoca / som_model['epocas'])

        # Se sigma for muito pequeno, garante que não seja zero para evitar divisão por zero na gaussiana
        if sigma < 1e-6: sigma = 1e-6

        # Embaralhar os dados a cada época
        np.random.shuffle(dados)

        for entrada in dados:
            bmu_i, bmu_j = achar_bmu(som_model, entrada)

            for i in range(som_model['linhas']):
                for j in range(som_model['colunas']):
                    coord_neuronio = (i, j)
                    dist_viz = distancia_vizinhanca_som((bmu_i, bmu_j), coord_neuronio)

                    # Atualiza apenas neurônios dentro do raio de vizinhança
                    if dist_viz <= sigma:
                        influencia = gaussiana(dist_viz, sigma)
                        pesos = som_model['grade'][i, j]

                        # Atualização dos pesos: pesos_novos = pesos_antigos + taxa * influencia * (entrada - pesos_antigos)
                        diferenca = entrada - pesos
                        ajuste = taxa * influencia * diferenca
                        som_model['grade'][i, j] = pesos + ajuste

def obter_som_clusters(som_model, dados):
    # Atribui cada ponto de dado ao seu BMU (Best Matching Unit)
    # O cluster ID pode ser simplesmente o índice linear do BMU na grade
    clusters = np.zeros(len(dados), dtype=int)
    for i, entrada in enumerate(dados):
        bmu_r, bmu_c = achar_bmu(som_model, entrada)
        clusters[i] = bmu_r * som_model['colunas'] + bmu_c
    return clusters

"""###Plotagem"""

# --- 4. SOM (Kohonen) Visualization ---
som_rows = 2
som_cols = 2
som_lr = 0.1
som_sigma = 1.0
som_epochs = 100

row_col_list = [(2,2),(4,4),(5,5),(2,5),(5,2)]
lr_list = [0.1,0.2,0.3]
sigma_list = [1.0,2.0,3.0]
epochs_list = [100,500]

for som_lr in lr_list:
  for som_sigma in sigma_list:
    for som_epochs in epochs_list:
      for som_rows, som_cols in row_col_list:

          som_model = inicializar_som(som_rows, som_cols, X_scaled.shape[1],
                                      taxa_aprendizado=som_lr, sigma=som_sigma, epocas=som_epochs)
          treinar_som(som_model, X_scaled)
          som_clusters = obter_som_clusters(som_model, X_scaled)

          df_som = df_base.copy()
          df_som['som_cluster'] = som_clusters

          g_som = sns.pairplot(df_som, hue='som_cluster', palette='tab10', corner=True)
          plt.suptitle(f"Kohonen (SOM) Clustering (Grid={som_rows}x{som_cols}, lr={som_lr}, epochs={som_epochs}, sigma={som_sigma}) - Iris Dataset", y=1.02)
          set_consistent_axis_limits_and_ticks(g_som, feature_names, x_limits)
          plt.show()